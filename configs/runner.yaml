runner:
  # mode
  visualize: False
  sub_mode: "subqa" # "description"
  # settings
  select_high_confidence: True
  train_recomposer_examplar: False
  threshold_lba: False
  vision_supple: False
  use_pre_generated_sub_q: True
  use_pre_generated_sub_a: True
  num_sub_qa_generate: 5
  num_sub_qa_select: 1
  num_pick_subq: 1
  num_frame_sampling: 3

  # model
  recomposer_name: "Salesforce/blip2-flan-t5-xl" # "Salesforce/instructblip-vicuna-7b" "LanguageBind/Video-LLaVA-7B-hf" "sevila"
  decomposer_name: "self" # ["self", "small", "base", "large", "xl", "xxl"]
  answerer_name: "Salesforce/blip2-flan-t5-xl" # None
  HF_HOME: /model/
  device_map: "cuda:0" # ['auto', 0, 1, ...]

  batch_size: 32 # xl: image:32, video: 16 or 12
  seed: 44

  # output & log
  output_dir: 'output/'
  print_freq: 50
  use_vqa_tool: False
  num_bin: 20
  num_heatmap_row: 10
  # conf_gap: 0.0
  debug: False

  sevila_cfg_pkl_path: 'SeViLA/cfg.pkl'
  flipped_vqa_cfg_pkl_path: 'flipped_vqa/cfg.pkl'


datasets:
  dataset_name: VQA_Introspect
  root_dir: /data/
  # num of data
  num_data: -1  # number of data to use
  split: 'val' # [train|val|test]
  # n_supple: 3

model:
  cache_dir: /model/ # ~/.cache/torch/hub/