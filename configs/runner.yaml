runner:
  # mode
  visualize: False
  sub_mode: "subqa" # "description"
  # settings
  select_high_confidence: True
  train_recomposer_examplar: False
  threshold_lba: False
  random_frame: False
  num_sub_qa_generate: 3
  num_sub_qa_select: 1

  # model
  recomposer_name: "Salesforce/blip2-flan-t5-xl" # "Salesforce/instructblip-vicuna-7b" "LanguageBind/Video-LLaVA-7B-hf" "sevila"
  decomposer_name: "self" # ["self", "small", "base", "large", "xl", "xxl"]
  answerer_name: "Salesforce/blip2-flan-t5-xl" # None
  HF_HOME: /data2/
  device_map: "cuda:0" # ['auto', 0, 1, ...]

  batch_size: 32 # xl: image:32, video: 16 or 12
  seed: 44

  # output & log
  output_dir: 'output/'
  print_freq: 50
  use_vqa_tool: False
  num_bin: 50
  num_heatmap_row: 10
  conf_gap: 0.1
  debug: False

  cfg_pkl_path: 'SeViLA/cfg.pkl'


datasets:
  dataset_name: VQA_Introspect
  root_dir: /data1/
  # num of data
  num_data: -1  # number of data to use
  split: 'val' # [train|val|test]
  n_supple: 3

model:
  cache_dir: /data2/ # ~/.cache/torch/hub/