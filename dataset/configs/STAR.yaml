

datasets:
  data_type: videos # [images|videos|features]
  n_frms: 4
  vqa_acc: False
  ann_paths:
    train:
      - STAR/sevila_style/train.json
    val:
      - STAR/sevila_style/val.json
      # - STAR/sub_questions_val.json # created by gpt
      # - STAR/sub_qas_val.json # created by gpt
      # - STAR/sub_qas_val_xl.json # created by xl
      - STAR/sub_qas_val_xl_Ktype.json # created by xl, Ktype

  vis_root: STAR/videos_480/

  vis_processor:
    train:
      name: "blip_video_eval"
      n_frms: 4
      image_size: 224
      min_scale: 0.9
      max_scale: 1.0
    eval:
      name: "blip_video_eval"
      n_frms: 4
      image_size: 224
      min_scale: 0.9
      max_scale: 1.0
  text_processor:
    train:
      name: "blip_question"
    eval:
      name: "blip_caption"
      
