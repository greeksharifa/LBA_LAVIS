{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ywjang/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ywjang/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
                        "  warn(\n"
                    ]
                }
            ],
            "source": [
                "import requests\n",
                "import os\n",
                "import argparse\n",
                "import json\n",
                "\n",
                "from omegaconf import OmegaConf\n",
                "from typing import Optional\n",
                "from PIL import Image\n",
                "\n",
                "from transformers import CLIPProcessor, CLIPModel\n",
                "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch import nn\n",
                "import torchvision\n",
                "from torchvision.io import read_video\n",
                "from torchvision.transforms import Resize, Normalize\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "from dataset.base_dataset import load_dataset\n",
                "from configs.config import Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_args():\n",
                "    parser = argparse.ArgumentParser(description='LBA method')\n",
                "    parser.add_argument(\"--cfg-path\", default='configs/runner.yaml', help=\"path to configuration file.\")\n",
                "    # verbose\n",
                "    parser.add_argument('--verbose', action='store_true', help='verbose')\n",
                "    \n",
                "    parser.add_argument(\n",
                "        \"--options\",\n",
                "        nargs=\"+\",\n",
                "        help=\"override some settings in the used config, the key-value pair \"\n",
                "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
                "        \"change to --cfg-options instead.\",\n",
                "    )\n",
                "    \n",
                "    args = parser.parse_args()\n",
                "    return args\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def video_to_tensor(video_path, target_size=(224, 224)):\n",
                "    # Read the video file\n",
                "    video, audio, info = read_video(video_path)\n",
                "    \n",
                "    # video tensor shape: (T, H, W, C)\n",
                "    # T: number of frames, H: height, W: width, C: channels (usually 3 for RGB)\n",
                "    \n",
                "    # Permute dimensions to (T, C, H, W) as expected by most PyTorch models\n",
                "    video = video.permute(0, 3, 1, 2)\n",
                "    \n",
                "    # Convert to float and scale to [0, 1]\n",
                "    # video = video.float() / 255.0\n",
                "    \n",
                "    # Resize the frames\n",
                "    resize = Resize(target_size)\n",
                "    video = resize(video)\n",
                "    \n",
                "    return video\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def read_image_using_clip(model, preprocess, image_paths):\n",
                "    if isinstance(image_paths, str):\n",
                "        image_paths = [image_paths]\n",
                "    \n",
                "    image_features = []\n",
                "    for image_path in image_paths:\n",
                "        pil_image = Image.open(image_path).convert('RGB')\n",
                "        import pdb; pdb.set_trace()\n",
                "        image = preprocess(images=pil_image).unsqueeze(0).to(device)\n",
                "\n",
                "        # Extract features\n",
                "        with torch.no_grad():\n",
                "            image_feature = model.encode_image(image)\n",
                "        \n",
                "        image_features.append(image_feature)        \n",
                "    \n",
                "    return torch.stack(image_features, dim=0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a1ef7d549be147b79cdab6fd953741bd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "Blip2VisionModel(\n",
                            "  (embeddings): Blip2VisionEmbeddings(\n",
                            "    (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
                            "  )\n",
                            "  (encoder): Blip2Encoder(\n",
                            "    (layers): ModuleList(\n",
                            "      (0-38): 39 x Blip2EncoderLayer(\n",
                            "        (self_attn): Blip2Attention(\n",
                            "          (dropout): Dropout(p=0.0, inplace=False)\n",
                            "          (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
                            "          (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
                            "        )\n",
                            "        (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
                            "        (mlp): Blip2MLP(\n",
                            "          (activation_fn): GELUActivation()\n",
                            "          (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
                            "          (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
                            "        )\n",
                            "        (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
                            "      )\n",
                            "    )\n",
                            "  )\n",
                            "  (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
                            ")"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "cache_dir = '/model/Salesforce'\n",
                "\n",
                "# clip_model_id = \"openai/clip-vit-large-patch14\"\n",
                "# clip_model = CLIPModel.from_pretrained(\n",
                "#     clip_model_id, \n",
                "#     cache_dir=os.path.join(cfg.model_cfg.cache_dir, clip_model_id.split('/')[0])\n",
                "# ).to(device)\n",
                "# clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
                "# clip_model.eval()\n",
                "\n",
                "model_id = 'Salesforce/blip2-flan-t5-xl'\n",
                "processor = Blip2Processor.from_pretrained(model_id)\n",
                "model = Blip2ForConditionalGeneration.from_pretrained(\n",
                "    model_id, \n",
                "    cache_dir=cache_dir\n",
                ").to(device)\n",
                "model.eval()\n",
                "blip2visionmodel = model.vision_model\n",
                "# Blip2VisionModel(\n",
                "#     model_id, \n",
                "#     cache_dir=os.path.join(cfg.model_cfg.cache_dir, model_id.split('/')[0])\n",
                "# ).to(device)\n",
                "blip2visionmodel.eval()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "datasets_cfg = {'dataset_name': 'DramaQA', 'num_data': -1, 'split': 'val', 'data_type': 'videos', 'n_frms': 4, 'vqa_acc': False, 'only_scene': False, 'ann_paths': {'train': ['/data/AnotherMissOh/AnotherMissOhQA_train_set.json', '/data/flipped_vqa/dramaqa/clipvitl14.pth'], 'val': ['/data/AnotherMissOh/AnotherMissOhQA_val_set.json', '/data/flipped_vqa/dramaqa/clipvitl14.pth']}, 'vis_root': '/data/AnotherMissOh/images/', 'vis_processor': {'train': {'name': 'alpro_video_train', 'n_frms': 5, 'image_size': 224, 'min_scale': 0.9, 'max_scale': 1.0}, 'eval': {'name': 'alpro_video_eval', 'n_frms': 5, 'image_size': 224, 'min_scale': 0.9, 'max_scale': 1.0}}, 'text_processor': {'train': {'name': 'blip_question'}, 'eval': {'name': 'blip_caption'}}}\n",
                "from omegaconf import OmegaConf\n",
                "datasets_cfg = OmegaConf.create(datasets_cfg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  3888/  3889 : AnotherMissOh15_039_0000\n",
                        "DramaQAEvalDataset\n",
                        "vis_processor :  None\n",
                        "text_processor :  None\n",
                        "vis_root :  /data/AnotherMissOh/images/\n",
                        "ann_paths :  ['/data/AnotherMissOh/AnotherMissOhQA_val_set.json', '/data/flipped_vqa/dramaqa/clipvitl14.pth']\n",
                        "type(self.annotation), len(self.annotation): <class 'list'> 3889\n",
                        "type(self.vis_features), len(self.vis_features): <class 'dict'> 23125\n",
                        "batch.keys(): dict_keys(['image', 'text_input', 'question_id', 'gt_ans', 'candidate_list', 'answer_sentence', 'type', 'vid'])\n",
                        "image               : 2 4 (768, 1024, 3)\n",
                        "text_input          : ['How is the relationship between Haeyoung1 and Dokyung when the two hug and kiss each other?', 'Why did Haeyoung1 lean against the wall when Haeyoung1 was walking in the alley with Dokyung?']\n",
                        "question_id         : [3205, 3200]\n",
                        "gt_ans              : [0, 3]\n",
                        "answer_sentence     : ['Haeyoung1 and Dokyung are in love and the two went through many things before starting to date.', 'Because Haeyoung1 tried to recall the time the two shared in the alley.']\n",
                        "type                : ['Level 3', 'Level 3']\n",
                        "vid                 : ['AnotherMissOh14_001_0000', 'AnotherMissOh14_001_0000']\n"
                    ]
                }
            ],
            "source": [
                "dataset = load_dataset(datasets_cfg, split='val')\n",
                "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=dataset.collater)\n",
                "\n",
                "vid_set = []\n",
                "for i, batch in enumerate(dataloader):\n",
                "    if i == 0:\n",
                "        print('batch.keys():', batch.keys())\n",
                "        for k, v in batch.items():\n",
                "            if hasattr(v, \"shape\"):\n",
                "                print(f'{k:<20s}: {v.shape}')\n",
                "            elif isinstance(v, list) and hasattr(v[0], \"shape\"):\n",
                "                print(f'{k:<20s}: {len(v)} {v[0].shape}')\n",
                "            elif isinstance(v, list) and isinstance(v[0], list) and hasattr(v[0][0], \"shape\"):\n",
                "                print(f'{k:<20s}: {len(v)} {len(v[0])} {v[0][0].shape}')\n",
                "            elif k != \"candidate_list\":\n",
                "                print(f'{k:<20s}: {v}')\n",
                "    else:\n",
                "        vid_set.extend(batch['vid'])\n",
                "        \n",
                "vid_set = list(set(vid_set))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def read_image_as_one_frame_video_to_tensor(image_paths, target_size=(224, 224)):\n",
                "    if isinstance(image_paths, str):\n",
                "        image_paths = [image_paths]\n",
                "    \n",
                "    totensor = torchvision.transforms.ToTensor()\n",
                "    \n",
                "    result = []\n",
                "    \n",
                "    for image_path in image_paths:\n",
                "        pil_image = Image.open(image_path).convert('RGB')\n",
                "        tensor = totensor(pil_image)\n",
                "        result.append(tensor)\n",
                "        \n",
                "    return torch.stack(result, dim=0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "processed: <class 'transformers.image_processing_base.BatchFeature'> <class 'list'> 4\n",
                        "processed['pixel_values'][0]: <class 'numpy.ndarray'> (3, 224, 224)\n",
                        "<class 'torch.Tensor'> torch.Size([4, 257, 1408])\n"
                    ]
                }
            ],
            "source": [
                "def extract(processor, blip2visionmodel, image_paths):\n",
                "    if isinstance(image_paths, str):\n",
                "        image_paths = [image_paths]\n",
                "    \n",
                "    # List[str] -> List[PIL.Image]\n",
                "    pil_image_list = [Image.open(image_path).convert('RGB') for image_path in image_paths]\n",
                "    \n",
                "    # -> <'BatchFeature'>['pixel_values']: List of torch.Tensor. 4 (3, 224, 224)\n",
                "    processed = processor(images=np.stack(pil_image_list, axis=0))\n",
                "    print('processed:', type(processed), type(processed['pixel_values']), len(processed['pixel_values']))\n",
                "    print(\"processed['pixel_values'][0]:\", type(processed['pixel_values'][0]), processed['pixel_values'][0].shape)\n",
                "    \n",
                "    totensor = torchvision.transforms.ToTensor()\n",
                "    tensor = torch.tensor(np.stack(processed['pixel_values'], axis=0)).to(device)\n",
                "    \n",
                "    # return tensor\n",
                "    \n",
                "    return blip2visionmodel(tensor, return_dict=True).last_hidden_state\n",
                "    \n",
                "    processed = torch.stack(processed['pixel_values'], dim=0)\n",
                "    return blip2visionmodel(processed['pixel_values'], return_dict=True).last_hidden_state\n",
                "\n",
                "\n",
                "vid = 'AnotherMissOh13_008_0313'\n",
                "image_paths = dataset.get_image_path(vid)\n",
                "feature = extract(processor, blip2visionmodel, image_paths)\n",
                "print(type(feature), feature.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([4, 257, 1408])"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "feature.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1928/2471 : AnotherMissOh13_008_0313\n",
                        "processed: <class 'transformers.image_processing_base.BatchFeature'> <class 'list'> 4\n",
                        "processed['pixel_values'][0]: <class 'numpy.ndarray'> (3, 224, 224)\n"
                    ]
                }
            ],
            "source": [
                "features = {}\n",
                "\n",
                "for i, vid in enumerate(vid_set):\n",
                "    if vid != 'AnotherMissOh13_008_0313':\n",
                "        continue\n",
                "    \n",
                "    print(f'{i+1}/{len(vid_set)} : {vid}')\n",
                "    if datasets_cfg.dataset_name == 'DramaQA':\n",
                "        image_paths = dataset.get_image_path(vid)\n",
                "        feature = extract(processor, blip2visionmodel, image_paths)\n",
                "        # feature = image_extract(processor, blip2visionmodel, image_paths)\n",
                "        \n",
                "        # features[vid] = read_image_as_one_frame_video_to_tensor(image_paths)\n",
                "        # features[vid] = read_image_using_clip(clip_model, clip_processor, image_paths)\n",
                "        # for img_path in image_paths:\n",
                "        #     # frms.append(Image.open(img_path))\n",
                "        #     frms.append(np.array(Image.open(img_path)))\n",
                "        # if len(frms) < cfg.datasets_cfg.n_frms:\n",
                "        #     # frms = [Image.new('RGB', frms[0].size)] * (self.n_frms - len(frms)) + frms\n",
                "        #     frms = [np.zeros_like(frms[0])] * (self.n_frms - len(frms)) + frms\n",
                "        features[vid] = feature\n",
                "    else:\n",
                "        vpath = os.path.join(cfg.datasets_cfg.vis_root, f'{vid}.mp4')\n",
                "        if os.path.exists(vpath):\n",
                "            features[vid] = torch.load(vpath)\n",
                "        else:\n",
                "            print(f'{vpath} not exists')\n",
                "\n",
                "\n",
                "# video = features['AnotherMissOh13_008_0313']\n",
                "# inputs = processor(video, questions, return_tensors=\"pt\", padding=True).to(\"cuda\")  # , torch.float16)\n",
                "\n",
                "# out = model.generate(**inputs)\n",
                "# print(out)\n",
                "# print(processor.batch_decode(out, skip_special_tokens=True))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'torch.Tensor'> torch.Size([4, 257, 1408])\n"
                    ]
                }
            ],
            "source": [
                "print(type(feature), feature.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "LBA_uncertainty_v2",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}